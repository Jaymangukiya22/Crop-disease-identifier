{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26124da",
   "metadata": {},
   "source": [
    "- In order to download dataset, go to Kaggle account -> API section -> Click on Create New API Token. This will download a file named kaggle.json.\n",
    "\n",
    "- Then go to colab and select the \"key\" on the left side, where click on \"add new secret\" where you need to provide Name and Value for KAGGLE_USERNAME and KAGGLE_KEY. Do this for both (you need to click \"add new secret twice, one for username and other for key\"). Make sure to tick the Notebook access option as well.\n",
    "\n",
    "- Here, these steps occur\n",
    "\n",
    "Step 0A: Setup: It installs the Augmentor library into your Colab environment. This gives us the tools needed for the main task.\n",
    "\n",
    "Step 0B: Data Download: It securely uses your Kaggle credentials to download the entire PlantVillage dataset into your Colab session. It then defines two critical paths:\n",
    "\n",
    "SOURCE_DIRECTORY: The location of the original, unbalanced grayscale images.\n",
    "\n",
    "OUTPUT_DIRECTORY: The new, empty folder where your final, balanced dataset will be built.\n",
    "\n",
    "Step 1: Analysis: The code acts like a census taker. It goes through every one of the 38 subfolders in the SOURCE_DIRECTORY, counts how many images are in each, and stores this information. It then finds the folder with the most images and sets that number as the TARGET_COUNT. This is the goal that all other folders must reach.\n",
    "\n",
    "Step 2: The Balancing Loop: This is the core of the operation and what is running right now. The code iterates through every single class (e.g., 'Tomato___Bacterial_spot', 'Potato___Early_blight', etc.). For each class, it performs the following sub-steps:\n",
    "\n",
    "Creates a new home: It makes a corresponding subfolder in the OUTPUT_DIRECTORY.\n",
    "\n",
    "Copies the originals: It copies all the original images from the source folder to this new folder. This ensures your final dataset contains all the original data.\n",
    "\n",
    "Makes a decision (This answers your next question): It checks if the number of images for this class is less than the TARGET_COUNT.\n",
    "\n",
    "Generates new images (if necessary): If the class is smaller than the target, it creates an Augmentor pipeline. This pipeline defines a set of random transformations (rotate, zoom, flip, change brightness/contrast). It then tells the pipeline to generate exactly the number of new images needed to reach the TARGET_COUNT. These new images are saved directly into the new output folder for that class.\n",
    "\n",
    "Skips if not needed: If the class is already at the target size, it does nothing and moves to the next class.\n",
    "\n",
    "Step 3: Final Verification: After the main loop is finished, this last step runs. It goes through your new OUTPUT_DIRECTORY and counts the images in every subfolder one last time. It then prints a final report to confirm that every single class has been successfully balanced to the TARGET_COUNT.\n",
    "\n",
    "- Running this code might take 30-40 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93939413",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle kagglehub -q\n",
    "\n",
    "import os\n",
    "from google.colab import userdata\n",
    "import kagglehub\n",
    "\n",
    "# Set Kaggle API credentials from Colab secrets (this part is correct)\n",
    "try:\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
    "    os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
    "except:\n",
    "    print(\"Kaggle credentials not found in Colab secrets. Please add them.\")\n",
    "    # You might want to stop the script here if credentials are required\n",
    "    # raise ValueError(\"Kaggle credentials not found.\")\n",
    "\n",
    "print(\"Downloading dataset...\")\n",
    "# This function correctly downloads and mounts the data as a folder\n",
    "path = kagglehub.dataset_download(\"abdallahalidev/plantvillage-dataset\")\n",
    "\n",
    "print(f\"Dataset is ready and mounted at the path: {path}\")\n",
    "\n",
    "# --- IMPORTANT ---\n",
    "# We have REMOVED the entire zipfile section because it is not needed.\n",
    "\n",
    "# Now, let's define the path to the grayscale images we will use for the project.\n",
    "# This SOURCE_DIRECTORY variable is what the next code blocks will use.\n",
    "SOURCE_DIRECTORY = os.path.join(path, 'plantvillage dataset', 'grayscale')\n",
    "\n",
    "# --- Verification Step ---\n",
    "# Let's check if this path is correct by listing its contents.\n",
    "print(\"\\nVerifying the source directory path...\")\n",
    "if os.path.exists(SOURCE_DIRECTORY):\n",
    "    print(f\"SUCCESS: The source directory is correctly set to:\\n'{SOURCE_DIRECTORY}'\")\n",
    "    print(\"\\nHere are some of the class folders inside:\")\n",
    "    # List the first 10 folders to confirm\n",
    "    print(os.listdir(SOURCE_DIRECTORY)[:10])\n",
    "else:\n",
    "    print(f\"ERROR: The directory was not found at '{SOURCE_DIRECTORY}'. Please check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db803d2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# THE DEFINITIVE AUGMENTOR WORKFLOW\n",
    "# This single cell will handle everything: setup, download, analysis, and balancing.\n",
    "# ==============================================================================\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# STEP 0A: SETUP AND INSTALLATION\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"--> Step 0A: Installing Augmentor and required libraries...\")\n",
    "# We use -q for a cleaner output, as we are confident in this installation.\n",
    "!pip install Augmentor kagglehub -q\n",
    "print(\"--> Done.\")\n",
    "\n",
    "import Augmentor\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from google.colab import userdata\n",
    "\n",
    "print(\"\\n------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# STEP 0B: DOWNLOAD DATA AND DEFINE PATHS\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"--> Step 0B: Downloading dataset and defining paths...\")\n",
    "\n",
    "# Set Kaggle API credentials\n",
    "try:\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
    "    os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
    "except Exception as e:\n",
    "    print(f\"    - Warning: Could not set Kaggle credentials from secrets: {e}\")\n",
    "\n",
    "# Download the dataset\n",
    "path = kagglehub.dataset_download(\"abdallahalidev/plantvillage-dataset\")\n",
    "\n",
    "# Define the source directory for our project (grayscale images)\n",
    "SOURCE_DIRECTORY = os.path.join(path, 'plantvillage dataset', 'grayscale')\n",
    "# Define where to save our new, balanced dataset\n",
    "OUTPUT_DIRECTORY = '/content/plant_village_balanced_augmentor/'\n",
    "\n",
    "if os.path.exists(SOURCE_DIRECTORY):\n",
    "    print(f\"--> SUCCESS: Source data is ready at '{SOURCE_DIRECTORY}'\")\n",
    "    print(f\"--> The new balanced dataset will be created at '{OUTPUT_DIRECTORY}'\")\n",
    "else:\n",
    "    print(f\"--> FATAL ERROR: Source directory was not found. Halting execution.\")\n",
    "    # This stops the script if the data isn't there\n",
    "    sys.exit()\n",
    "\n",
    "print(\"\\n------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# STEP 1: ANALYZE CLASS IMBALANCE\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"--> Step 1: Analyzing class distribution...\")\n",
    "\n",
    "class_counts = defaultdict(int)\n",
    "for class_folder in os.listdir(SOURCE_DIRECTORY):\n",
    "    folder_path = os.path.join(SOURCE_DIRECTORY, class_folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        num_images = len(glob.glob(os.path.join(folder_path, '*.JPG')))\n",
    "        class_counts[class_folder] = num_images\n",
    "\n",
    "if not class_counts:\n",
    "    print(\"--> FATAL ERROR: No class folders found in source directory.\")\n",
    "    sys.exit()\n",
    "else:\n",
    "    sorted_classes = sorted(class_counts.items(), key=lambda item: item[1])\n",
    "    TARGET_COUNT = sorted_classes[-1][1]\n",
    "    print(f\"--> Analysis complete. Target image count per class is: {TARGET_COUNT}\")\n",
    "    print(f\"    (Based on the largest class: '{sorted_classes[-1][0]}')\")\n",
    "\n",
    "print(\"\\n------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# STEP 2: PERFORM BALANCING WITH AUGMENTOR\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"--> Step 2: Starting dataset balancing using Augmentor...\")\n",
    "\n",
    "# Create the main output directory\n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# Loop through all the original class folders\n",
    "for class_name, current_count in sorted_classes:\n",
    "    print(f\"\\n--- Processing class: {class_name} ({current_count} images) ---\")\n",
    "    \n",
    "    # Define paths for this specific class\n",
    "    source_class_path = os.path.join(SOURCE_DIRECTORY, class_name)\n",
    "    output_class_path = os.path.join(OUTPUT_DIRECTORY, class_name)\n",
    "    os.makedirs(output_class_path, exist_ok=True)\n",
    "    \n",
    "    # --- 1. Copy original images to the new directory ---\n",
    "    # Augmentor works best by creating a new, separate dataset\n",
    "    original_image_paths = glob.glob(os.path.join(source_class_path, '*.JPG'))\n",
    "    for img_path in original_image_paths:\n",
    "        shutil.copy(img_path, output_class_path)\n",
    "    print(f\"    - Copied {len(original_image_paths)} original images.\")\n",
    "\n",
    "    # --- 2. Perform augmentation if needed ---\n",
    "    if current_count < TARGET_COUNT:\n",
    "        num_to_generate = TARGET_COUNT - current_count\n",
    "        print(f\"    - Augmentation needed. Generating {num_to_generate} new images...\")\n",
    "        \n",
    "        # Create an Augmentor Pipeline that points to the *final* output directory\n",
    "        p = Augmentor.Pipeline(output_class_path, output_directory=\".\", save_format=\"JPG\")\n",
    "        \n",
    "        # --- Define the augmentation operations ---\n",
    "        # These are added to the pipeline. Probabilities control how often they are applied.\n",
    "        p.rotate(probability=0.7, max_left_rotation=20, max_right_rotation=20)\n",
    "        p.zoom(probability=0.4, min_factor=0.9, max_factor=1.2)\n",
    "        p.flip_left_right(probability=0.5)\n",
    "        p.flip_top_bottom(probability=0.5)\n",
    "        p.random_contrast(probability=0.6, min_factor=0.8, max_factor=1.2)\n",
    "        p.random_brightness(probability=0.6, min_factor=0.8, max_factor=1.2)\n",
    "        \n",
    "        # --- Execute the pipeline ---\n",
    "        # The 'sample' function generates the new images on disk.\n",
    "        p.sample(num_to_generate, multi_threaded=False) # multi_threaded=False is more stable in Colab\n",
    "        \n",
    "        print(f\"    - Augmentation complete for '{class_name}'.\")\n",
    "    else:\n",
    "        print(\"    - Class is at target size. No augmentation needed.\")\n",
    "\n",
    "print(\"\\n------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# STEP 3: FINAL VERIFICATION\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"--> Step 3: Verifying the new balanced dataset...\")\n",
    "\n",
    "all_classes_balanced = True\n",
    "for class_folder in os.listdir(OUTPUT_DIRECTORY):\n",
    "    count = len(os.listdir(os.path.join(OUTPUT_DIRECTORY, class_folder)))\n",
    "    print(f\"    - {class_folder}: {count} images\")\n",
    "    if count != TARGET_COUNT:\n",
    "        all_classes_balanced = False\n",
    "\n",
    "print(\"\\n--- VERIFICATION SUMMARY ---\")\n",
    "if all_classes_balanced:\n",
    "    print(f\"✅ SUCCESS! All classes have been balanced to {TARGET_COUNT} images.\")\n",
    "    print(f\"Your balanced dataset is ready at: '{OUTPUT_DIRECTORY}'\")\n",
    "else:\n",
    "    print(\"❌ NOTICE: Some classes are not at the target count. Please review the output.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
